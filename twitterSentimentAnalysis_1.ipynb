{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import unicodedata as udata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.4\n",
      "2.2.4\n",
      "1.17.2\n",
      "0.24.2\n",
      "3.4\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(numpy.__version__)\n",
    "print(pd.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"TwitterSentimentAnalysis.csv\", encoding='latin-1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentiment', 'id', 'date', 'query', 'user', 'text'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "id           0\n",
       "date         0\n",
       "query        0\n",
       "user         0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2242505815</td>\n",
       "      <td>Fri Jun 19 12:16:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AngeLiiTaCullen</td>\n",
       "      <td>seeing Emilie and Rob kissing even just for a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2066083434</td>\n",
       "      <td>Sun Jun 07 09:51:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JussCallMeDaddy</td>\n",
       "      <td>juss got home. went 2 thunder valley last nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1989211351</td>\n",
       "      <td>Mon Jun 01 00:43:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Mousethefiend</td>\n",
       "      <td>hooray! i finally get to go to bed! got to get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1970277671</td>\n",
       "      <td>Sat May 30 03:11:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CiaraSlave</td>\n",
       "      <td>@JPopMp3s I am, sis!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1836399659</td>\n",
       "      <td>Mon May 18 07:55:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ifahmi</td>\n",
       "      <td>@chibialfa waaah, udah jalan pulang Fa. Mandy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  2242505815  Fri Jun 19 12:16:30 PDT 2009  NO_QUERY   \n",
       "1          4  2066083434  Sun Jun 07 09:51:25 PDT 2009  NO_QUERY   \n",
       "2          0  1989211351  Mon Jun 01 00:43:45 PDT 2009  NO_QUERY   \n",
       "3          4  1970277671  Sat May 30 03:11:30 PDT 2009  NO_QUERY   \n",
       "4          0  1836399659  Mon May 18 07:55:44 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  AngeLiiTaCullen  seeing Emilie and Rob kissing even just for a ...  \n",
       "1  JussCallMeDaddy  juss got home. went 2 thunder valley last nigh...  \n",
       "2    Mousethefiend  hooray! i finally get to go to bed! got to get...  \n",
       "3       CiaraSlave                              @JPopMp3s I am, sis!   \n",
       "4           ifahmi  @chibialfa waaah, udah jalan pulang Fa. Mandy ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"id\", \"date\", \"query\", \"user\"], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>seeing Emilie and Rob kissing even just for a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>juss got home. went 2 thunder valley last nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hooray! i finally get to go to bed! got to get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@JPopMp3s I am, sis!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@chibialfa waaah, udah jalan pulang Fa. Mandy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  seeing Emilie and Rob kissing even just for a ...\n",
       "1          4  juss got home. went 2 thunder valley last nigh...\n",
       "2          0  hooray! i finally get to go to bed! got to get...\n",
       "3          4                              @JPopMp3s I am, sis! \n",
       "4          0  @chibialfa waaah, udah jalan pulang Fa. Mandy ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(df.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4</td>\n",
       "      <td>@mileycyrus this is silly but i had a dance tr...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4</td>\n",
       "      <td>@mia_kitty I think your right about the song w...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>4</td>\n",
       "      <td>Thx 4 the kind words, props, &amp;amp; re-blips  @...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>4</td>\n",
       "      <td>@selenagomez Hello Selena, I love &amp;quot;Tell M...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>4</td>\n",
       "      <td>Guys, checkout ABC's #Wipeout! It's totally FU...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>@Sealbroken Classic Nick Walker &amp;amp; ome grea...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>@FrankieTheSats @MollieOfficial @Rochellewisem...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>4</td>\n",
       "      <td>@BlackWidowDiy ..... OMG it was ALOT of fun!!!...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0</td>\n",
       "      <td>@ayatoshirosan ...later to say that you just c...</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>4</td>\n",
       "      <td>@dee__cee im assuming &amp;quot;lg traning&amp;quot; i...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                               text  \\\n",
       "69           4  @mileycyrus this is silly but i had a dance tr...   \n",
       "94           4  @mia_kitty I think your right about the song w...   \n",
       "187          4  Thx 4 the kind words, props, &amp; re-blips  @...   \n",
       "201          4  @selenagomez Hello Selena, I love &quot;Tell M...   \n",
       "235          4  Guys, checkout ABC's #Wipeout! It's totally FU...   \n",
       "274          0  @Sealbroken Classic Nick Walker &amp; ome grea...   \n",
       "399          0  @FrankieTheSats @MollieOfficial @Rochellewisem...   \n",
       "486          4  @BlackWidowDiy ..... OMG it was ALOT of fun!!!...   \n",
       "500          0  @ayatoshirosan ...later to say that you just c...   \n",
       "510          4  @dee__cee im assuming &quot;lg traning&quot; i...   \n",
       "\n",
       "     pre_clean_len  \n",
       "69             145  \n",
       "94             143  \n",
       "187            142  \n",
       "201            143  \n",
       "235            147  \n",
       "274            142  \n",
       "399            141  \n",
       "486            145  \n",
       "500            146  \n",
       "510            148  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pre_clean_len > 140].head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'        # remove @ mentions from tweets\n",
    "pat2 = r'https?://[^ ]+'        # remove URLs from tweets\n",
    "combined_pat = r'|'.join((pat1, pat2)) #addition of pat1 and pat2\n",
    "www_pat = r'www.[^ ]+'         # remove URLs from tweets\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",   # converting words like isn't to is not\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(text):  # define tweet_cleaner function to clean the tweets\n",
    "    soup = BeautifulSoup(text, 'lxml')    # create beautiful soup object\n",
    "    souped = soup.get_text()   # get only text from the tweets \n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")    # remove utf-8-sig code\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed) # calling combined_pat\n",
    "    stripped = re.sub(www_pat, '', stripped) #remove URLs\n",
    "    lower_case = stripped.lower()      # converting all into lower case\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case) # converting words like isn't to is not\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)       # will replace # by space\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1] # Word Punct Tokenize and only consider words whose length is greater than 1\n",
    "    return (\" \".join(words)).strip() # join the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.392333984375e-05\n",
      "10000 3.4728739261627197\n",
      "20000 6.913676023483276\n",
      "30000 10.346076726913452\n",
      "40000 13.782924890518188\n"
     ]
    }
   ],
   "source": [
    "#Note that we have 1600000 instances. But processing so many instances will take a very very long time.\n",
    "#Hence, restricting to rather 50000 instances.\n",
    "limit=50000\n",
    "import time; \n",
    "ms = time.time()\n",
    "#nums = [0,400000,800000,1200000,1600000] # used for batch processing tweets\n",
    "#nums = [0, 9999]\n",
    "clean_tweet_texts = [] # initialize list\n",
    "for i in range(0,limit): # batch process 1.6 million tweets \n",
    "    if i % 10000==0:\n",
    "        print(i, time.time()-ms)\n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))  # call tweet_cleaner function and pass parameter as all the tweets to clean the tweets and append cleaned tweets into clean_tweet_texts list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jyotikhetan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [] # initialize list for tokens\n",
    "for word in clean_tweet_texts:  # for each word in clean_tweet_texts\n",
    "    word_tokens.append(word_tokenize(word)) #tokenize word in clean_tweet_texts and append it to word_tokens list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jyotikhetan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = [] # initialize list df1 to store words after lemmatization\n",
    "from nltk.stem import WordNetLemmatizer # import WordNetLemmatizer from nltk.stem\n",
    "lemmatizer = WordNetLemmatizer() # create an object of WordNetLemmatizer\n",
    "for l in word_tokens: # for loop for every tokens in word_token\n",
    "    b = [lemmatizer.lemmatize(q) for q in l] #for every tokens in word_token lemmatize word and giev it to b\n",
    "    df1.append(b) #append b to list df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df1 =[] # initialize list clean_df1 to join word tokens after lemmatization\n",
    "for c in df1:  # for loop for each list in df1\n",
    "    a = \" \".join(c) # join words in list with space in between and give it to a\n",
    "    clean_df1.append(a) # append a to clean_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame(clean_df1,columns=['text']) # convert clean_tweet_texts into dataframe and name it as clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['clean_len'] = [len(t) for t in clean_df.text] # Again make a new coloumn in the dataframe and name it as clean_len which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, clean_len]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[clean_df.clean_len > 140].head(10) # again check if any tweet is more than 140 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>seeing Emilie and Rob kissing even just for a ...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>juss got home. went 2 thunder valley last nigh...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hooray! i finally get to go to bed! got to get...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@JPopMp3s I am, sis!</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@chibialfa waaah, udah jalan pulang Fa. Mandy ...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  pre_clean_len\n",
       "0          0  seeing Emilie and Rob kissing even just for a ...             90\n",
       "1          4  juss got home. went 2 thunder valley last nigh...            120\n",
       "2          0  hooray! i finally get to go to bed! got to get...             94\n",
       "3          4                              @JPopMp3s I am, sis!              21\n",
       "4          0  @chibialfa waaah, udah jalan pulang Fa. Mandy ...             58"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2 = [] # initialize list\n",
    "for i in range(0,limit): # batch process 1.6 million tweets \n",
    "    target2.append(df['sentiment'][i])\n",
    "clean_df['target']=target2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(50000,)\n",
      "{0, 4}\n",
      "dict_values([24901, 25099])\n"
     ]
    }
   ],
   "source": [
    "X = clean_df.text # get all the text in x variable\n",
    "y = clean_df.target # get all the sentiments into y variable\n",
    "print(X.shape) #print shape of x\n",
    "print(y.shape) # print shape of y\n",
    "from collections import Counter\n",
    "print(set(y)) # equals to list(set(words))\n",
    "print(Counter(y).values()) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split #from sklearn.cross_validation import train_test_split to split the data into training and tesing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state= 1) # split the data into traing and testing set where ratio is 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(analyzer = \"word\", ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(X_train) \n",
    "X_train_dtm = vect.transform(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # import Multinomial Naive Bayes model from sklearn.naive_bayes\n",
    "nb = MultinomialNB(alpha = 10) # get object of Multinomial naive bayes model with alpha parameter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=10, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train_dtm, y_train)# fit our both training data tweets as well as their sentiments to the multinomial naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7616001157828197"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score  # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = nb, X = X_train_dtm, y = y_train, cv = 10) # do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb = nb.predict(X_test_dtm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7593"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics # import metrics from sklearn\n",
    "metrics.accuracy_score(y_test, y_pred_nb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4202,  770],\n",
       "       [1637, 3391]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix # import confusion matrix from the sklearn.metrics\n",
    "confusion_matrix(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # import Logistic Regression model from sklearn.linear_model\n",
    "logisticRegr = LogisticRegression(C = 1.1) # get object of logistic regression model with cost parameter = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotikhetan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(X_train_dtm, y_train)# fit our both traing data tweets as well as its sentiments to the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotikhetan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7755001159171947"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = logisticRegr, X = X_train_dtm, y = y_train, cv = 10) # do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lg = logisticRegr.predict(X_test_dtm)  # predict the sentiments of testing data tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7751"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics # import metrics from sklearn\n",
    "metrics.accuracy_score(y_test, y_pred_lg) # measure the accuracy of our model on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4008,  964],\n",
       "       [1285, 3743]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix # import confusion matrix from the sklearn.metrics\n",
    "confusion_matrix(y_test, y_pred_lg) # plot the confusion matrix between our predicted sentiments and the original testing data sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC # import SVC model from sklearn.svm\n",
    "svm_clf = LinearSVC(random_state=0) # get object of SVC model with random_state parameter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train_dtm, y_train)# fit our both traing data tweets as well as its sentiments to the SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7802751534859471"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score  # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = svm_clf, X = X_train_dtm, y = y_train, cv = 10)# do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm_clf.predict(X_test_dtm)  # predict the sentiments of testing data tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7789"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics  # import metrics from sklearn\n",
    "metrics.accuracy_score(y_test, y_pred_svm)  # measure the accuracy of our model on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4017,  955],\n",
       "       [1256, 3772]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix # import confusion matrix from the sklearn.metrics\n",
    "confusion_matrix(y_test, y_pred_svm)# plot the confusion matrix between our predicted sentiments and the original testing data sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dtc = DecisionTreeClassifier(random_state = 100, max_depth=3, min_samples_leaf=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dtc.fit(X_train_dtm, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5826252135172008"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "accuracies = cross_val_score(estimator = clf_dtc, X = X_train_dtm, y = y_train, cv = 10)\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dtc = clf_dtc.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5739"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_dtc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1228, 3744],\n",
       "       [ 517, 4511]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred_dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
